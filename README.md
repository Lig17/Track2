
#  Baby Dragon Hatchling (BDH) with Synaptic Scaffolding

### Inference-Time Learning via Hebbian Synapses (Post-Transformer Architecture)

---

## Overview

This repository extends the **Baby Dragon Hatchling (BDH)** architecture with **Synaptic Scaffolding** â€” a biologically inspired mechanism that enables **learning during inference without backpropagation**.

Unlike Transformers, which rely on a **temporary KV-cache** and **frozen weights**, BDH stores memory **directly in synapses** using **Hebbian plasticity**.
This allows the model to **adapt, retain, and reuse knowledge across sessions**.

> **Core idea:**
> **State lives in synapses, not in prompts or external memory.**

---

## What Is Synaptic Scaffolding?

Synaptic Scaffolding introduces three key mechanisms:

### 1ï¸âƒ£ Hebbian Fast Weights (Ïƒ)

* Connections between **co-active neurons strengthen during inference**
* Classic rule: *â€œNeurons that fire together, wire togetherâ€*

### 2ï¸âƒ£ Metaplasticity (H)

* Frequently used synapses **forget more slowly**
* Important memories become **structurally protected**

### 3ï¸âƒ£ Cross-Session Persistence

* Synaptic state can be **saved and reloaded**
* Knowledge survives **beyond a single prompt or context window**

âœ… Enables **native continual learning**
âŒ No fine-tuning
âŒ No external retrieval system

---

##  Architecture Summary

| Component        | Transformer             | BDH + Synaptic Scaffolding |
| ---------------- | ----------------------- | -------------------------- |
| Memory           | KV-Cache (temporary)    | Synapses (Ïƒ)               |
| Learning         | Training-time only      | Inference-time             |
| Forgetting       | Immediate after session | Controlled decay           |
| Scaling          | O(TÂ²) attention         | O(T) local updates         |
| Interpretability | Low                     | High (sparse synapses)     |

---

## Repository Structure

```
.
â”œâ”€â”€ bdh.py                 # BDH model + Synaptic Scaffolding
â”œâ”€â”€ train.py               # Baseline training (Tiny Shakespeare)
â”œâ”€â”€ baseline_test.py       # Baseline inference (no learning)
â”œâ”€â”€ scaffolding_test.py    # Synaptic exposure + persistence test
â”œâ”€â”€ input.txt              # Tiny Shakespeare dataset
â”œâ”€â”€ bdh_baseline.pt        # Saved baseline weights
â”œâ”€â”€ glip_memory.pt         # Saved synaptic memory (example)
â””â”€â”€ README.md
```

---

## Environment Setup

**Python â‰¥ 3.10 recommended**

```bash
python -m venv bdh_env
source bdh_env/bin/activate
pip install torch numpy requests
```

âš ï¸ CPU-only runs are supported but slower.

---

## Step 1: Baseline Training

Train BDH normally (**no synaptic learning yet**):

```bash
python train.py
```

### What this does:

* Trains BDH on **Tiny Shakespeare**
* Establishes **slow structural weights**
* Saves a **baseline language model**

**Expected output:**

```
Step: 0 loss ...
Step: 100 loss ...
Training done, now generating a sample
```

---

## Step 2: Baseline Test (No Learning)

Test the frozen model:

```bash
python baseline_test.py
```

**Example output:**

```
What is a glip?

DUKE:
I will tends, and's the caure too arms.
```

ðŸ‘‰ The model **does not know** what a *glip* is.

---

## Step 3: Synaptic Scaffolding Test

### Learning During Inference

Run inference-time learning:

```bash
python scaffolding_test.py
```

### What happens internally:

* The model is exposed repeatedly to a new fact

  > *â€œA glip is a small blue bird.â€*
* Sparse neurons co-activate
* Synapses (Ïƒ) strengthen via **Hebbian updates**
* Synaptic history (H) reduces decay on frequent paths
* Synaptic state is **saved to disk**

**Example console output:**

```
Synapse update triggered, activity = 0.36
Exposure done. Synapses saved.
{'sigma_norm': 4884.6, 'stiff_synapses': 0.0016, 'avg_decay': 0.0099}
```

---

## Step 4: Cross-Session Recall

In a fresh model instance, load synapses:

```python
model.attn.load_synapses("glip_memory.pt")
```

**Prompt:**

```
What is a glip?
```

The modelâ€™s **internal structure has changed** â€” even without retraining.

>  **Key point:**
> Learning is demonstrated via **structural change**, not perfect text fluency.

---

## Diagnostics (Important for Evaluation)

Inspect synaptic health:

```python
model.attn.get_diagnostics()
```

Returns:

* **sigma_norm** â†’ total memory formed
* **stiff_synapses** â†’ fraction of hardened connections
* **avg_decay** â†’ effective forgetting rate

These metrics provide **quantitative evidence of learning**.

---

## What This Demonstrates

âœ… Learning without backpropagation
âœ… Memory beyond the context window
âœ… No external retrieval system
âœ… Biologically plausible plasticity
âœ… Interpretable internal state

Directly addresses:

* Transformer amnesia
* Catastrophic forgetting
* KV-cache scaling limits

---

## Experimental Status

* Research prototype
* Text output may be noisy (expected)
* **Structural metrics are the primary signal**
* Designed for **Frontier / Research track** evaluation

---

## Hackathon Relevance

This project aligns with **Path B: Continuous Learning & Synaptic Dynamics**.

> This is **not** a chatbot demo.
> It is a **systems-level exploration of post-Transformer intelligence**.

---

## References

* Pathway â€” *Baby Dragon Hatchling (BDH)*
* *The Dragon Hatchling: The Missing Link Between Transformers and the Brain* (arXiv)
* Hebbian Learning & Metaplasticity (Neuroscience)

---

## Acknowledgements

Inspired by the original **BDH work by Pathway** and the broader community exploring **biologically grounded AI**.

---
